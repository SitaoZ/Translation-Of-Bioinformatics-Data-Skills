第六章 生物信息学数据

到目前为止，我们关注了很多生物信息学项目开始之前的东西：组织目录结构，unix, 远程登录机器和版本控控制。然而，我们忽略了生物信息项目中很重要的东西：数据。
数据对任何生物信息学项目来说时必须的。我们后面会通过提炼大量的数据来了解复杂的生物学系统。不幸的是，许多处理小数据和中等数据的任务是对大量的基因组数据是一个挑战。这些挑战包括：
检索数据
不管是下载大量的测序数据还是大量访问某个网站下载特殊的文件，检索数据需要特定的工具和技巧
保证数据的完整性
通过网络转移大量的数据会导致加大数据中断的风险，会导致得到错误分析结果。因此，在下游分析之前，我们需要保证数据的完整性。这个工具也可以用来验证我们在分析中使用到了正确的数据版本。
压缩数据
生物信息学中的数据通常很大，因此我们需要进行数据的压缩。因此，对压缩数据的处理是生信中一个必备的技巧。


检索数据
假设你被告知你的项目的测序已经完成。你已经从你的测序中心下载了6条lane的Illumina数据.通过网页下载这些数据是不可取的。网页不是被设计来下载大量数据集的。此外，你需要将测序数据下载到服务器，而不是你的链接网络的本地设备。为了实现，我们需要用SSH登陆到你的服务器，直接使用命令行来进行数据下载。这一部分我们将看一些这样的工具。

使用wget和curl下载数据
最常用的两个下载数据的命令就是，使用wget和curl.依赖自己的系统，这些命令有可能没有被安装，你可以通过命令行来安装这几个包，比如，Homebrew和apt-get.尽管wget和curl在功能上有一定的相似性，他们的侧重点是不同的，以至于我们两者都用的很频繁。

wget 
wget被用来快速下载文件，例如，下载人类基因组版本GRCh37的22号染色体。
```bash
$ wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/chromosomes/chr22.fa.gz
```
wget 下载文件到你当前的目录，提供一个可使用的进程指标。注意，这个22号染色体的链接以http（超文本转移协定的简称）开头。wget也能够下载FTP的链接（文件转移协定的简称）。一般情况下，FTP比http更加适合大文件的下载，UCSC推荐使用wget下载。

因为UCSC网站一般用来提供人类的基因组版本，我们不需要授权就能获得。然而，如果你要下载数据来自于LIMS系统，你就需要账号和密码来授权下载。对于简单的HTTP和wget授权，你可以使用 wget --user= 和 --ask-passwd 选项。有些时候你需要更加复杂的授权，这种情况下，你就需要联系系统管理员获取权限。

wget 的一个优势就是可以循环下载数据。当下载时，使用--recursive或者-r，wegt就会递归下载链接。默认情况下，只会下载五个文件，但是可以通过--level -l 进行自定义。
递归下载页面全部数据是非常有用的。例如，一个实验室网站有许多GTF文件，我们需要下载，我们可以使用如下的命令：
```bash
wget --accept "*.gtf" --no-directories --recursive --no-parent \
http://genomics.someuniversity.edu/labsite/annotation.html
```
但是要注意，wget递归下载是激进的。如果不受约束，wget会下载全部的数据，在指定的--level下。在前面的例子中，我们限制wget采取两种方式，--no-parent 阻止wget下载数据上一级目录的数据。--accept "*.gtf"指定只下载这类文件名的数据。

练习注意点，当使用wget递归下载选项时，它会占用很多网络资源，重复加载远程的服务器。在一些情况下，如果你下载的很多很快的话，远程服务器会关掉你的IP。如果你计划下载很多数据，你最好知道下载多少数据不会被屏蔽掉IP，wget --limit-rate 能够实现下载的限速。
wget 是非常好的工具。前面的例子仅仅只展示了他能力的一点点。表格6-1列出来常用的一些参数，man wget 会全部列出。

Curl 

curl是轻量级的wget.wget 非常适用于下载HTTP和FTP的数据，下载网页数据可以使用递归的参数。curl使用起来类似，默认情况下会将文件输出到标准输出。下载人类22号染色体，像wget那样，我们可以使用如下参数：
```bash
curl http://[...]/goldenPath/hg19/chromosomes/chr22.fa.gz > chr22.fa.gz
```
注意，我们使用了截断的URL，所以这个例子也适用于网页。这个URL地址和我们之前wget使用的一样。
如果你不喜欢重定向curl的输出，可以使用 -O <文件名> 将结果写入到文件中。如果你忽视了文件名参数，curl 将会使用远程服务器使用的文件名。

curl 也有自己的优势，和wget相比，它可以使用更多的协议传输文件。包括SFTP(secure FTP)和SCP（secure copy）。curl一个特别好的特征就是可以使用-L/--location。使用这个参数，curl可以下载最终的文件，而不是重定向网页本身。最后，curl也是一个库，意味着不仅仅作为一个命令行程序，它也可以被其他软件使用，例如RCurl和pycurl.

Rsync and Secure Copy (scp)
wget 和 curl是命令行是快速下载文件的非常好的工具。但是对大量的duty任务不是最佳的方案。例如，假设你的合作者需要你目录中大量的测序文件，这些目录可能被Git忽略。一个好的工具就是使用Rsync下载整个目录。
Rsync是这类任务的最好的选择有如下几个原因。第一，Rsync非常快，因为它只会发送文件版本之间的差异（当文件存在或者部分存在），同时也会在传输中对文件进行压缩。第二，Rysnc有一个文档属性来保护links,改变时间戳，权限，所有者，和其他一些文件属性。Rsync还有一些特征和参数来处理不同情景的脚本。例如对远程文件做一些操作。
rsync的一些基本语法 是 rsync source destination, 这里的源你想拷贝的文件或者目录，目的地就是你想把文件放在那里的目的地。源或者目的能使用 user@host:/path/to/directory/远程指定.


